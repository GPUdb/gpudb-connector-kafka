# Kinetica Kafka Connector JUnit tests

There are two separate JUnit suites for Kinetica Kafka Connector: 
- JUnit tests provided with Connector as part of *kafka-connect-kinetica* project, which depend 
on a running Kinetica instance and mock Kafka services. These tests are used to check 
Kinetica DB service avaiability to run Kafka Connector successfully and verify Connector
configuration. JUnit tests in *kafka-connect-kinetica* project follow the "happy path" when 
mapping schemas and processing Kafka records. 
- JUnit tests provided in a sister project *test-connect* also depend on a running Kinetica DB 
instance, but some of the helper classes are configured to run as stand-alone applications that 
can access Kafka stack and post messages to Kafka topics in various formats, including 
plain String, schemaless JSON, schemaless Oracle Golden Gate JSON, avro encoded JSON, 
and producing messages with multiple schema versions of the same object 
(to test Kafka Connector's implementation of schema evolution).

For a more detailed documantation of available formats, test utilities and 
integration tests that require both Kinetica DB and Kafka stack (Confluent platform)
running, please refer to [test-connect/README.md][TEST_LOC] project
[TEST_LOC]: <test-connect/README.md>


## Maven build test goal Prerequisites

Current setup uses Kinetica addresses: localhost:9191, localhost:8080/gadmin.
Kafka Kinetica Connector uses ports 8090 and 8089 (as Sink and Source ports), if the 
port is undefined, it uses 8083 by default. When starting Kinetica and Kafka on the same machine,
make sure there is no port conflict (some configurations use ports 8088 and 8082 in default
settings which can lead to Confluent rest and ksql services failing).  

Start Kinetica DB and GAdmin (please refer to Kinetica Documentation for [detailed instructions][KINETICA_DOCS_START]) 
[KINETICA_DOCS_START]: <https://www.kinetica.com/docs/gpudbAdmin/services.html>
[KAFKA_DOC_START]: <https://kafka.apache.org/quickstart>

Make sure that you have a running Kinetica instance, and that its URL and user credentials 
are properly configured in files `config/quickstart-kinetica-sink.properties`
and `config/quickstart-kinetica-source.properties`.


Clone and build the *kafka-connect-kinetica* project as follows: 

```sh
git clone https://github.com/gisfederal/kinetica-connector-kafka
cd kinetica-connector-kafka/kafka-connect-kinetica
mvn clean compile package
```

At the end of running maven build, you'll see a tally report:

```sh
Results :

Tests run: 41, Failures: 0, Errors: 0, Skipped: 2
```

If any of the tests failed, please review the Surefire reports available in 
`kafka-connect-kinetica/target/surefire-reports` folder.

Verify that running JUnit tests as a goal of Maven build created the following tables in Kinetica DB *TEST* collection:

```
KafkaConnectorTest
KafkaConnectorTest2
outKafkaConnectorTest
outRX_TX_FIN_EVENT
outtable
outtableA
outTX_TP_FIN_EVENT
tableA
```

## Datapump Test Utility

This JUnit tests suite also provides a standalone datapump utility that can be used as a part of integration test 
between Kafka stack and Kinetica DB. TestDataPump is configured to generate fake "TweetRecord" activity, it
creates Kinetica DB tables `KafkaConnectorTest` and `KafkaConnectorTest2` and inserts batches of records 
at regular intervals (see `batch-size` and `delay-seconds` configuration options).

```sh
usage: TestDataPump [options] [URL]
 -c,--configFile <path>      Relative path to configuration file.
 -d,--delay-time <seconds>   Seconds between batches.
 -h,--help                   Show Usage
 -n,--batch-size <count>     Number of records in a batch.
 -t,--total-batches <count>  Number of batches to insert.
```

The below example (built for kafka 2.0.0 amd kinetica 7.0.0.0) runs the datapump with default options and will 
insert into Kinetica table batches of 10 records every 3 seconds.

```sh
java -cp kafka-2.0.0-connector-kinetica-7.0.0.0-tests.jar:kafka-2.0.0-connector-kinetica-7.0.0.0-jar-with-dependencies.jar \
    com.kinetica.kafka.TestDataPump http://localhost:9191
```
You can also provide a relative path to Kinetica DB instance configuration file that contains URL, username, password and timeout:

```sh
java -cp kafka-2.0.0-connector-kinetica-7.0.0.0-tests.jar:kafka-2.0.0-connector-kinetica-7.0.0.0-jar-with-dependencies.jar \
    com.kinetica.kafka.TestDataPump -c config/quickstart-kinetica-sink.properties
```


## System Test

This test will demonstrate the *Kinetica Kafka Connector* source and sink in standalone mode. The
standalone mode should be used only for testing. You should use  [distributed mode][DIST_MODE] for a
production deployment. This test uses the records autogenerated through TestDataPump to create a Kafka
topic, then read those from Kafka topic into a new Kinetica table (cretaed with prefix "out").

[DIST_MODE]: <https://docs.confluent.io/current/connect/managing.html#configuring-connectors>

Create configuration files `connect-standalone-sink.properties` and `connect-standalone-source.properties`
based on example below. Make sure rest.port for sink and source files is set to different values.
This example may require modifications (editing IP addresses, ports, local paths) to fit your environment.

```
# This should point to your Kafka broker
bootstrap.servers = localhost:9092

offset.storage.file.filename=/tmp/connect.offsets
offset.flush.interval.ms = 5000

# These ports must not be used by other processes on the host.
# source rest port
rest.port = 8089
# sink rest port
# rest.port = 8090

# Provide the path to your connector jar here:
# plugin.path = /opt/connect-test

# Key is stored in commit log with JSON schema.
key.converter = org.apache.kafka.connect.json.JsonConverter
key.converter.schemas.enable = true

# Value is stored in commit log with JSON schema.
value.converter = org.apache.kafka.connect.json.JsonConverter
value.converter.schemas.enable = true

# Disable schemas for internal key/value parameters:
internal.key.converter = org.apache.kafka.connect.json.JsonConverter
internal.key.converter.schemas.enable = false
internal.value.converter = org.apache.kafka.connect.json.JsonConverter
internal.value.converter.schemas.enable = false
```

Create a configuration file `quickstart-kinetica-source.properties` for the source connector:

```
# Connector API required config parameters
name = TwitterSourceConnector
connector.class = com.kinetica.kafka.KineticaSourceConnector
tasks.max = 1

# Kinetica specific config parameters
kinetica.url = http://localhost:9191
kinetica.table_names = KafkaConnectorTest,KafkaConnectorTest2
kinetica.timeout = 1000
kinetica.topic_prefix = Tweets.
```

Create a configuration file `quickstart-kinetica-sink.properties` for the sink connector:

```
name = TwitterSinkConnector
topics = Tweets.KafkaConnectorTest,Tweets.KafkaConnectorTest2
connector.class = com.kinetica.kafka.KineticaSinkConnector
tasks.max = 1

# Kinetica specific config
kinetica.url = http://localhost:9191
kinetica.collection_name = TEST
kinetica.table_prefix = out_
kinetica.timeout = 1000
kinetica.batch_size = 100
```

The rest of this system test will require three terminal windows.

* In terminals 1 and 2, start *zookeeper* and *kafka*:

```sh
$ cd <path/to/Kafka>
$ bin/zookeeper-server-start.sh config/zookeeper.properties 
$ bin/kafka-server-start.sh config/server.properties
```


* In terminals 3 and 4, start kafka source and sink connectors:

```sh
$ connect-standalone connect-standalone-source.properties quickstart-kinetica-source.properties 
```

```sh
$ connect-standalone connect-standalone-sink.properties quickstart-kinetica-sink.properties
```


* Verify that data is copied to tables `out_KafkaConnectorTest` and `out_KafkaConnectorTest2`.


To test schemaless JSON format, in `connect-standalone-source.properties` and 
`connect-standalone-sink.properties` files set 

```
key.converter.schemas.enable=false
value.converter.schemas.enable=false
```

After running the Integration test you should see the following Kinetica DB tables in *TEST* Collection:
```
KafkaConnectorTest
KafkaConnectorTest2
out_KafkaConnectorTest
``` 
and Kafka topics `Tweets.KafkaConnectorTest`, `Tweets.KafkaConnectorTest2` on provided Kafka stack.
